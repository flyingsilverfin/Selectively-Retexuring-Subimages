{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"/local/scratch/js2173/pytorch/Selectively-Retexuring-Subimages/submodules/pix2pixHD\") # access submodules\n",
    "sys.path = [p for p in sys.path if not p.startswith('/local/scratch') or p.startswith('/local/scratch/js2173')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.data_loader import CreateDataLoader\n",
    "from models.models import create_model\n",
    "\n",
    "from options.train_options import TrainOptions\n",
    "from options.test_options import TestOptions\n",
    "import util.util as util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# for testing images in here\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py', '-f', '/run/user/3879/jupyter/kernel-92340d76-1ca7-40a9-9e16-072abe703f3a.json']\n",
      "------------ Options -------------\n",
      "aspect_ratio: 1.0\n",
      "batchSize: 1\n",
      "checkpoints_dir: ./checkpoints\n",
      "cluster_path: features_clustered_010.npy\n",
      "dataroot: ./datasets/cityscapes/\n",
      "display_winsize: 512\n",
      "feat_num: 3\n",
      "fineSize: 512\n",
      "gpu_ids: [0]\n",
      "how_many: 50\n",
      "instance_feat: False\n",
      "isTrain: False\n",
      "label_feat: False\n",
      "label_nc: 35\n",
      "loadSize: 1024\n",
      "load_features: False\n",
      "max_dataset_size: inf\n",
      "model: pix2pixHD\n",
      "nThreads: 2\n",
      "n_blocks_global: 9\n",
      "n_blocks_local: 3\n",
      "n_clusters: 10\n",
      "n_downsample_E: 4\n",
      "n_downsample_global: 4\n",
      "n_local_enhancers: 1\n",
      "name: label2city\n",
      "nef: 16\n",
      "netG: global\n",
      "ngf: 64\n",
      "niter_fix_global: 0\n",
      "no_flip: False\n",
      "no_instance: False\n",
      "norm: instance\n",
      "ntest: inf\n",
      "output_nc: 3\n",
      "phase: test\n",
      "resize_or_crop: scale_width\n",
      "results_dir: ./results/\n",
      "serial_batches: False\n",
      "tf_log: False\n",
      "use_dropout: False\n",
      "which_epoch: latest\n",
      "-------------- End ----------------\n"
     ]
    }
   ],
   "source": [
    "# this can only be run once since we're in ipython rather than from an actual command line to parse args from\n",
    "opt = TestOptions()\n",
    "print(sys.argv)\n",
    "tmp = sys.argv\n",
    "sys.argv = [tmp[0]]\n",
    "opt = opt.parse(save=False)\n",
    "sys.argv = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py', '-f', '/run/user/3879/jupyter/kernel-92340d76-1ca7-40a9-9e16-072abe703f3a.json']\n",
      "------------ Options -------------\n",
      "batchSize: 1\n",
      "beta1: 0.5\n",
      "checkpoints_dir: ./checkpoints\n",
      "continue_train: False\n",
      "cos_decay: False\n",
      "cos_decay_update_iters: 100\n",
      "dataroot: ./datasets/cityscapes/\n",
      "debug: False\n",
      "display_freq: 100\n",
      "display_winsize: 512\n",
      "feat_num: 3\n",
      "fineSize: 512\n",
      "gpu_ids: [0]\n",
      "instance_feat: False\n",
      "isTrain: True\n",
      "label_feat: False\n",
      "label_nc: 35\n",
      "lambda_feat: 10.0\n",
      "loadSize: 1024\n",
      "load_features: False\n",
      "load_pretrain: \n",
      "lr: 0.0002\n",
      "max_dataset_size: inf\n",
      "model: pix2pixHD\n",
      "nThreads: 2\n",
      "n_blocks_global: 9\n",
      "n_blocks_local: 3\n",
      "n_clusters: 10\n",
      "n_downsample_E: 4\n",
      "n_downsample_global: 4\n",
      "n_layers_D: 3\n",
      "n_local_enhancers: 1\n",
      "name: label2city\n",
      "ndf: 64\n",
      "nef: 16\n",
      "netG: global\n",
      "ngf: 64\n",
      "niter: 100\n",
      "niter_decay: 100\n",
      "niter_fix_global: 0\n",
      "no_flip: False\n",
      "no_ganFeat_loss: False\n",
      "no_html: False\n",
      "no_instance: False\n",
      "no_lsgan: False\n",
      "no_vgg_loss: False\n",
      "norm: instance\n",
      "num_D: 2\n",
      "output_nc: 3\n",
      "phase: train\n",
      "pool_size: 0\n",
      "print_freq: 100\n",
      "resize_or_crop: scale_width\n",
      "save_epoch_freq: 10\n",
      "save_latest_freq: 1000\n",
      "serial_batches: False\n",
      "started_epoch: None\n",
      "tf_log: False\n",
      "use_dropout: False\n",
      "which_epoch: latest\n",
      "-------------- End ----------------\n",
      "CustomDatasetDataLoader\n",
      "dataset [AlignedDataset] was created\n"
     ]
    }
   ],
   "source": [
    "train_opt = TrainOptions()\n",
    "print(sys.argv)\n",
    "tmp = sys.argv\n",
    "sys.argv = [tmp[0]]\n",
    "train_opt = train_opt.parse()\n",
    "sys.argv = tmp\n",
    "\n",
    "train_opt.instance_feat = True # put in features to steer generation\n",
    "train_opt.name = 'label2city_512p_feat'\n",
    "train_opt.dataroot = '/local/scratch/js2173/pytorch/Selectively-Retexuring-Subimages/submodules/pix2pixHD/datasets/cityscapes/'\n",
    "train_opt.checkpoints_dir = '/local/scratch/js2173/pytorch/Selectively-Retexuring-Subimages/submodules/pix2pixHD/checkpoints/'\n",
    "train_opt.results = '/local/scratch/js2173/pytorch/Selectively-Retexuring-Subimages/submodules/pix2pixHD/results/'\n",
    "#train_opt.no_flip = True\n",
    "\n",
    "\n",
    "\n",
    "# get test images\n",
    "train_data_loader = CreateDataLoader(train_opt)\n",
    "train_dataset = train_data_loader.load_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # filename we're looking for\n",
    "# cobblestone_road_image = \"jena_000034_000019\"\n",
    "# cobblestone_data = None\n",
    "# for (i, data) in enumerate(train_dataset):\n",
    "#     if i % 100 == 0:\n",
    "#         print(\"Scanning file number: \", i)\n",
    "#     if cobblestone_road_image in data['path'][-1]:\n",
    "#         print(\"Found cobblestone image!\")\n",
    "#         cobblestone_data = data\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "opt.nThreads = 1   # test code only supports nThreads = 1\n",
    "opt.batchSize = 1  # test code only supports batchSize = 1\n",
    "opt.serial_batches = True  # no shuffle\n",
    "opt.no_flip = True  # no flip\n",
    "opt.instance_feat = True # put in features to steer generation\n",
    "opt.name = 'label2city_512p_feat'\n",
    "opt.dataroot = '/local/scratch/js2173/pytorch/Selectively-Retexuring-Subimages/submodules/pix2pixHD/datasets/cityscapes/'\n",
    "opt.checkpoints_dir = '/local/scratch/js2173/pytorch/Selectively-Retexuring-Subimages/submodules/pix2pixHD/checkpoints/'\n",
    "opt.results = '/local/scratch/js2173/pytorch/Selectively-Retexuring-Subimages/submodules/pix2pixHD/results/'\n",
    "\n",
    "\n",
    "# get test images\n",
    "data_loader = CreateDataLoader(opt)\n",
    "dataset = data_loader.load_data()\n",
    "# create model\n",
    "model = create_model(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datas = enumerate(dataset)\n",
    "print(dataset.dataset.dataset_size)\n",
    "n, im0 = next(datas)\n",
    "label_map = Variable(im0['label'])\n",
    "size = label_map.size()\n",
    "oneHot_size = (size[0], opt.label_nc, size[2], size[3])\n",
    "input_label = torch.cuda.FloatTensor(torch.Size(oneHot_size)).zero_()\n",
    "input_label_2 = input_label.scatter_(1, label_map.data.long().cuda(), 1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {\n",
    "0: 'unlabeled',\n",
    "1: 'ego vehicle',\n",
    "2: 'rectification border',\n",
    "3: 'out of roi',\n",
    "4:'static',\n",
    "5:'dynamic' ,\n",
    "6:'ground',\n",
    "7:'road',\n",
    "8:'sidewalk',\n",
    "9:'parking',\n",
    "10:'rail track',\n",
    "11: 'building',\n",
    "12:'wall',\n",
    "13:'fence',\n",
    "14:'guard rail',\n",
    "15:'bridge',\n",
    "16:'tunnel',\n",
    "17:'pole',\n",
    "18:'polegroup',\n",
    "19:'traffic light',\n",
    "20:'traffic sign',\n",
    "21:'vegetation',\n",
    "22:'terrain',\n",
    "23:'sky',\n",
    "24:'person',\n",
    "25:'rider',\n",
    "26:'car',\n",
    "27:'truck',\n",
    "28:'bus',\n",
    "29:'caravan',\n",
    "30:'trailer',\n",
    "31:'train',\n",
    "32:'motorcycle',\n",
    "33:'bicycle',\n",
    "-1: 'license plate'\n",
    "}\n",
    "\n",
    "lab_to_id = { \n",
    "'unlabeled' : 0 ,\n",
    " 'ego vehicle' : 1 ,\n",
    " 'rectification border' : 2 ,\n",
    " 'out of roi' : 3 ,\n",
    "'static' : 4 ,\n",
    "'dynamic'  : 5 ,\n",
    "'ground' : 6 ,\n",
    "'road' : 7 ,\n",
    "'sidewalk' : 8 ,\n",
    "'parking' : 9 ,\n",
    "'rail track' : 10 ,\n",
    " 'building' : 11 ,\n",
    "'wall' : 12 ,\n",
    "'fence' : 13 ,\n",
    "'guard rail' : 14 ,\n",
    "'bridge' : 15 ,\n",
    "'tunnel' : 16 ,\n",
    "'pole' : 17 ,\n",
    "'polegroup' : 18 ,\n",
    "'traffic light' : 19 ,\n",
    "'traffic sign' : 20 ,\n",
    "'vegetation' : 21 ,\n",
    "'terrain' : 22 ,\n",
    "'sky' : 23 ,\n",
    "'person' : 24 ,\n",
    "'rider' : 25 ,\n",
    "'car' : 26 ,\n",
    "'truck' : 27 ,\n",
    "'bus' : 28 ,\n",
    "'caravan' : 29 ,\n",
    "'trailer' : 30 ,\n",
    "'train' : 31 ,\n",
    "'motorcycle' : 32 ,\n",
    "'bicycle' : 33 ,\n",
    " 'license plate' : -1\n",
    "}\n",
    "\n",
    "def custom_inference(pix2pixHD, label, inst, feat_map):\n",
    "    # Encode Inputs        \n",
    "    input_label, inst_map, _, _ = pix2pixHD.encode_input(Variable(label), Variable(inst), infer=True)\n",
    "    # sample clusters from precomputed features             \n",
    "    #feat_map = pix2pixHD.sample_features(inst_map)\n",
    "    input_concat = torch.cat((input_label, feat_map), dim=1)                        \n",
    "    fake_image = pix2pixHD.netG.forward(input_concat)\n",
    "    return fake_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# color stuff for plotting feature cluster centers\n",
    "import random\n",
    "\n",
    "def get_random_color(pastel_factor = 0.5):\n",
    "    return [(x+pastel_factor)/(1.0+pastel_factor) for x in [random.uniform(0,1.0) for i in [1,2,3]]]\n",
    "\n",
    "def color_distance(c1,c2):\n",
    "    return sum([abs(x[0]-x[1]) for x in zip(c1,c2)])\n",
    "\n",
    "def generate_new_color(existing_colors,pastel_factor = 0.5):\n",
    "    max_distance = None\n",
    "    best_color = None\n",
    "    for i in range(0,100):\n",
    "        color = get_random_color(pastel_factor = pastel_factor)\n",
    "        if not existing_colors:\n",
    "            return color\n",
    "        best_distance = min([color_distance(color,c) for c in existing_colors])\n",
    "        if not max_distance or best_distance > max_distance:\n",
    "            max_distance = best_distance\n",
    "            best_color = color\n",
    "    return best_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Plot ALL the features for a given class, see if it forms a continuous 3D curve/clusters?\n",
    "\n",
    "# for testing, get the cluster centers\n",
    "clusters_path = os.path.join(opt.checkpoints_dir, opt.name, opt.cluster_path) \n",
    "features_clustered = np.load(clusters_path).item()\n",
    "\n",
    "print(features_clustered)\n",
    "\n",
    "extrema_clustered_path = os.path.join(opt.checkpoints_dir, opt.name, 'features_extrema_010.npy')\n",
    "features_extrema_clustered = np.load(extrema_clustered_path).item()\n",
    "\n",
    "#opt.cluster_path = \"features_extrema_010.npy\"\n",
    "#features_clustered = features_extrema_clustered\n",
    "\n",
    "fig = plt.figure(1)\n",
    "ax = Axes3D(fig)\n",
    "colors = [get_random_color()]\n",
    "\n",
    "\n",
    "classes = ['building', 'road', 'pole']\n",
    "for cat in classes:\n",
    "    class_id = lab_to_id[cat]\n",
    "    points = features_clustered[class_id]\n",
    "    print(cat, points)\n",
    "    colors.append(generate_new_color(colors))\n",
    "    for pt in points:\n",
    "        ax.scatter(*pt, color=colors[-1])\n",
    "ax.set_xlabel('X Label')\n",
    "ax.set_ylabel('Y Label')\n",
    "ax.set_zlabel('Z Label')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_features(model, image, inst):\n",
    "    return model.encode_features(image, inst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cobblestone_image = cobblestone_data['image']\n",
    "# print(cobblestone_data)\n",
    "# cobblestone_inst = cobblestone_data['inst']\n",
    "# cobblestone_features = get_features(model, cobblestone_image, cobblestone_inst)\n",
    "# print(cobblestone_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "labels = im0['label'].clone()\n",
    "instances = im0['inst'].clone()\n",
    "\n",
    "\n",
    "# Just to test, attempt rewriting a semantic label as another to see what is generated!\n",
    "\n",
    "# target = 21\n",
    "# source = 7\n",
    "\n",
    "# # target = torch.Tensor([target])\n",
    "# # target = target.expand(labels.size())\n",
    "\n",
    "# mask = labels == source\n",
    "# labels[mask] = target\n",
    "\n",
    "# mask = instances == source\n",
    "# instances[mask] = target\n",
    "\n",
    "\n",
    "input_label, inst_map, _, _ = model.encode_input(Variable(labels), Variable(instances), infer=True)\n",
    "base_feat_map = model.sample_features(inst_map)\n",
    "semantic_region = 'road'\n",
    "semantic_region_id = lab_to_id[semantic_region]\n",
    "mask = (instances == semantic_region_id).cuda()\n",
    "mask = mask[0][0]\n",
    "\n",
    "\n",
    "cat = semantic_region\n",
    "class_id = lab_to_id[cat]\n",
    "features_list = features_clustered[class_id]\n",
    "\n",
    "# features_list = np.array([\n",
    "#     [0.5, -0.1, 0.5],\n",
    "#     [1.0, -1.0, 0.9],\n",
    "#     [-1.0, 1.0, -1.0],\n",
    "#     [ 10.0, -0.16616863,  0.06818795]\n",
    "# ])\n",
    "\n",
    "features_list = np.array([\n",
    "    [0.40335375, -0.37605467, 0.53809792],\n",
    "    [ 0.33693951,  0.04128251,  0.11454804],\n",
    "    [ 0.72480464,  0.05364915,  0.40521583]\n",
    "])\n",
    "\n",
    "#features_list =(np.random.rand(20, 3) - 0.5)*10\n",
    "\n",
    "for desired_feature in features_list:\n",
    "    print(desired_feature)\n",
    "    feat_map = base_feat_map.clone()\n",
    "    feat_map_copy = feat_map.clone()\n",
    "\n",
    "    # overwrite entire layer with each feature index, mask later\n",
    "    for (i, val) in enumerate(desired_feature):\n",
    "        val = torch.Tensor([val])\n",
    "        val = val.expand(feat_map_copy[0,i].size())\n",
    "        feat_map_copy[0, i] = val\n",
    "    \n",
    "    # mask feature map off this\n",
    "    for (i, layer) in enumerate(feat_map[0]):\n",
    "        layer[mask] = feat_map_copy[0, i][mask]\n",
    "    #feat_map = base_feat_map\n",
    "    \n",
    "    result = custom_inference(model, labels, instances, feat_map)\n",
    "    img_tensor = result.data[0]\n",
    "    img = util.tensor2im(img_tensor)\n",
    "    plt.figure(figsize=(20,10))\n",
    "    imgplot = plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_features(model, labels, instances):\n",
    "    input_label, inst_map, _, _ = model.encode_input(Variable(labels), Variable(instances), infer=True)\n",
    "    return model.sample_features(inst_map)\n",
    "        \n",
    "# End Goal: \n",
    "# Take in Region name, Segmentation, Texture (as image and instance map for now),\n",
    "# output image with texture applied to region\n",
    "def pix2pixHD_retexture(region_name, label_orig, inst_orig, texture_image, texture_inst=None, model=None, previous_features=None):\n",
    "\n",
    "\n",
    "    if model is None:\n",
    "        # TODO instantiate pix2pixHD\n",
    "        print(\"TODO\")\n",
    "        return\n",
    "    \n",
    "    # check texture image conforms to required pixel dimensions\n",
    "    if texture_image.size()[2:] != label_orig.size()[2:]:\n",
    "        print(\"Texture image has size: \", texture_image.size(),)\n",
    "        print(\"Input label map has size: \", label_orig.size(), )\n",
    "        print(\"Currently do not support mismatched pixel sizes\")\n",
    "        # TODO implement crop/resize to allow different sizes\n",
    "        # TODO look into encoder architecture to see how it deals with size\n",
    "        return\n",
    "    \n",
    "    # ID of region we want to remap texture of\n",
    "    target_region_id = lab_to_id[region_name]\n",
    "    \n",
    "    \n",
    "    # sample features for this label, inst\n",
    "    if texture_inst is None:\n",
    "        # create a single instance over the entire image of the required category\n",
    "        # for feeding into the encoder... not sure how well this will work\n",
    "        single_inst_value = torch.Tensor([target_region_id]).expand(inst_orig.size())\n",
    "        texture_inst = single_inst_value\n",
    "        \n",
    "    texture_features = model.encode_features(texture_image, texture_inst)\n",
    "    #print(texture_features)\n",
    "    print(\"Taking feature for ID: \", target_region_id, \"({})\".format(region_name))\n",
    "    target_feature = texture_features[target_region_id][0][:3]\n",
    "    print(\"=> Target feature: \", target_feature)\n",
    "    \n",
    "    # just to make sure we don't modify anything in place accidentally\n",
    "    labels = label_orig.clone()\n",
    "    instances = inst_orig.clone()\n",
    "\n",
    "\n",
    "    input_label, inst_map, _, _ = model.encode_input(Variable(labels), Variable(instances), infer=True)\n",
    "    \n",
    "    if previous_features is None:\n",
    "        feat_map = model.sample_features(inst_map).clone()\n",
    "    else:\n",
    "        feat_map = previous_features.clone()\n",
    "    \n",
    "    input_concat = torch.cat((input_label, feat_map), dim=1)\n",
    "    original_generated = model.netG.forward(input_concat)\n",
    "    \n",
    "    # get mask of target instances\n",
    "    mask = (instances == target_region_id).cuda()\n",
    "    mask = mask[0][0]\n",
    "    \n",
    "    # get copy of feature map\n",
    "    feat_map_copy = feat_map.clone()\n",
    "\n",
    "    # overwrite entire layer with each feature index, mask later\n",
    "    for (i, val) in enumerate(target_feature):\n",
    "        val = torch.Tensor([val])\n",
    "        val = val.expand(feat_map_copy[0,i].size())\n",
    "        feat_map_copy[0, i] = val\n",
    "    \n",
    "    # mask feature map off this\n",
    "    for (i, layer) in enumerate(feat_map[0]):\n",
    "        layer[mask] = feat_map_copy[0, i][mask]\n",
    "    #feat_map = base_feat_map\n",
    "    \n",
    "    # concateate feature map and input labels\n",
    "    input_concat = torch.cat((input_label, feat_map), dim=1)                        \n",
    "    modified_generated = model.netG.forward(input_concat)\n",
    "\n",
    "    return (original_generated, modified_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "texture_opt = TrainOptions()\n",
    "print(sys.argv)\n",
    "tmp = sys.argv\n",
    "sys.argv = [tmp[0]]\n",
    "texture_opt = texture_opt.parse(save=False)\n",
    "sys.argv = tmp\n",
    "texture_opt.phase = 'texture' # retrieves correct folders automatically\n",
    "texture_opt.instance_feat = True # put in features to steer generation\n",
    "texture_opt.name = 'label2city_512p_feat'\n",
    "texture_opt.dataroot = '/local/scratch/js2173/pytorch/Selectively-Retexuring-Subimages/submodules/pix2pixHD/datasets/cityscapes/'\n",
    "texture_opt.checkpoints_dir = '/local/scratch/js2173/pytorch/Selectively-Retexuring-Subimages/submodules/pix2pixHD/checkpoints/'\n",
    "texture_opt.results = '/local/scratch/js2173/pytorch/Selectively-Retexuring-Subimages/submodules/pix2pixHD/results/'\n",
    "texture_opt.no_flip = True\n",
    "\n",
    "# get test images\n",
    "texture_data_loader = CreateDataLoader(texture_opt)\n",
    "texture_dataset = texture_data_loader.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_opt = TestOptions()\n",
    "print(sys.argv)\n",
    "tmp = sys.argv\n",
    "sys.argv = [tmp[0]]\n",
    "eval_opt = eval_opt.parse(save=False)\n",
    "sys.argv = tmp\n",
    "eval_opt.phase = 'eval' # retrieves correct folders automatically\n",
    "eval_opt.instance_feat = True # put in features to steer generation\n",
    "eval_opt.name = 'label2city_512p_feat'\n",
    "eval_opt.dataroot = '/local/scratch/js2173/pytorch/Selectively-Retexuring-Subimages/submodules/pix2pixHD/datasets/cityscapes/'\n",
    "eval_opt.checkpoints_dir = '/local/scratch/js2173/pytorch/Selectively-Retexuring-Subimages/submodules/pix2pixHD/checkpoints/'\n",
    "eval_opt.results = '/local/scratch/js2173/pytorch/Selectively-Retexuring-Subimages/submodules/pix2pixHD/results/'\n",
    "eval_opt.no_flip = True\n",
    "\n",
    "# get test images\n",
    "eval_data_loader = CreateDataLoader(eval_opt)\n",
    "eval_dataset = eval_data_loader.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evals = list(enumerate(eval_dataset))\n",
    "textures = list(enumerate(texture_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_group(image_tensors, layout_shape, figsize=(20,10)):\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    for (i, im) in enumerate(image_tensors):\n",
    "        ax = fig.add_subplot(*layout_shape, i+1)\n",
    "        ax.set_title('')\n",
    "        plt.imshow(util.tensor2im(im))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_im(image_tensor, normalize=2.0, figsize=(20,10)):\n",
    "    # borrowed from util.tensor2im with normalization modification\n",
    "    image_numpy = image_tensor.cpu().float().numpy()\n",
    "    image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / normalize * 255.0\n",
    "    image_numpy = np.clip(image_numpy, 0, 255)\n",
    "    if image_numpy.shape[2] == 1:        \n",
    "        image_numpy = image_numpy[:,:,0]\n",
    "    img = image_numpy.astype(np.uint8)\n",
    "    #img = util.tensor2im(image_tensor)\n",
    "    plt.figure(figsize=figsize)\n",
    "    imgplot = plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for (i, data) in textures:\n",
    "    show_im(data['image'][0], normalize=2.0, figsize=(10,5))\n",
    "    #show_im(data['inst'][0], normalize=35)\n",
    "    #show_im(data['label'][0], normalize=35)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# texture_0_data = textures[4][1]\n",
    "# eval_0_data = evals[0][1]\n",
    "# result = pix2pixHD_retexture('road', \n",
    "#                     eval_0_data['label'],\n",
    "#                     eval_0_data['inst'],\n",
    "#                     texture_0_data['image'], \n",
    "#                     texture_inst=texture_0_data['inst'], \n",
    "#                     model=model)\n",
    "# unretextured = result[0]\n",
    "# retextured = result[1]\n",
    "# show_im(unretextured.data[0])\n",
    "# show_im(retextured.data[0])\n",
    "# show_im(texture_0_data['image'][0])\n",
    "\n",
    "for (i, eval_datum) in evals[0:2]:\n",
    "    generator_features = sample_features(model, eval_datum['label'], eval_datum['inst'])\n",
    "    for (j, texture_datum) in textures:\n",
    "        texture_inst = texture_datum['inst']\n",
    "        if 'yellow_brick' in texture_datum['path'][-1]:\n",
    "            texture_inst = None\n",
    "        result = pix2pixHD_retexture('road', \n",
    "                                     eval_datum['label'],\n",
    "                                     eval_datum['inst'],\n",
    "                                     texture_datum['image'], \n",
    "                                     texture_inst=texture_inst, \n",
    "                                     model=model,\n",
    "                                    previous_features=generator_features)\n",
    "        unretextured = result[0]\n",
    "        retextured = result[1]\n",
    "        show_group([unretextured.data[0], retextured.data[0], texture_datum['image'][0]],\n",
    "                   layout_shape=(3,1),\n",
    "                   figsize=(40,30)\n",
    "                    )\n",
    "#         show_im(unretextured.data[0])\n",
    "#         show_im(retextured.data[0])\n",
    "#         show_im(texture_datum['image'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
